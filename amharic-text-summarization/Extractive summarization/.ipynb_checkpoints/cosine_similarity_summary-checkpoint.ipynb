{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19f1555-625c-445d-b342-461a9f8d00cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yohannes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = open('stopwords.txt', encoding='utf-8').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eff0c22-e802-46da-a97e-d85f21c89b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('../../cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3674b640-736d-4824-a305-456c05c22ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>የአማራ ብሄራዊ ክልል መንግስት መግለጫ</td>\n",
       "      <td>የሀገራችን ህዝቦች በአብሮነትና በአንድነት በህብረ ብሄራዊነት ለበርካታ አ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>አዴፓዎች ለዶክተር አብይ ግልፅ ቅድመ ሁኔታዎች ማስቀመጥ አለባቸው ግርማ ካሳ</td>\n",
       "      <td>በታገቱ ተማሪዎችና በሌሎች የአገር ደህንነትና መከላከያ ጉዳዮች ዙሪያ አዴ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>በተማሪዎች ላይ የተፈፀመውን የእገታ ድርጊት የሚያወግዙ ሰላማዊ ሰልፎች በ...</td>\n",
       "      <td>ባህር ዳር ሰላማዊ ሰልፎቹ በደምቢ ዶሎ ዩኒቨርሲቲ ሲማሩ የነበሩና የታገቱ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>በአቶ ገዱ አንዳርጋቸው የተመራ የልኡካን ቡድን ወደ አሜሪካ ተጓዘ</td>\n",
       "      <td>በኢፌዲሪ የውጭ ጉዳይ ሚኒስትር አቶ ገዱ አንዳርጋቸው የተመራ የኢትዮጵያ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ጦቢያ ግጥምን በጃዝ   መላኩ ታረቀኝ</td>\n",
       "      <td>ጦቢያ ግጥምን በጃዝ  መላኩ ታረቀኝ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ዶክተር አብይ ሱዳንን አሻገረ   የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው</td>\n",
       "      <td>ዶክተር አብይ ሱዳንን አሻገረ  የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>በነገራችን ላይ አንዳርጋቸው ፅጌ</td>\n",
       "      <td>በነገራችን ላይ አንዳርጋቸው ፅጌ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ</td>\n",
       "      <td>ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>የአሜሪካ ድምፅ ራዲዮ ከአቶ ንጉሱ ጥላሁን እና ከሆኑት አቶ ነቢያት ጌታቸ...</td>\n",
       "      <td>የአሜሪካ ድምፅ ራዲዮ የጠቅላይ ሚኒስትር ፅህፈት ቤት ፕሬስ ሴክሬታሪያት ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ይድርስ በኢትዮጵያ ለምትገኙ የዩኒቨርስቲ ተማሪዎች</td>\n",
       "      <td>በገክርስቶስ አባይጥር  ቀን  አመተ ምህረትከሁሉ አስቀድሜ ሰላምና ጤና እ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           የአማራ ብሄራዊ ክልል መንግስት መግለጫ   \n",
       "1   አዴፓዎች ለዶክተር አብይ ግልፅ ቅድመ ሁኔታዎች ማስቀመጥ አለባቸው ግርማ ካሳ   \n",
       "2  በተማሪዎች ላይ የተፈፀመውን የእገታ ድርጊት የሚያወግዙ ሰላማዊ ሰልፎች በ...   \n",
       "3          በአቶ ገዱ አንዳርጋቸው የተመራ የልኡካን ቡድን ወደ አሜሪካ ተጓዘ   \n",
       "4                            ጦቢያ ግጥምን በጃዝ   መላኩ ታረቀኝ   \n",
       "5       ዶክተር አብይ ሱዳንን አሻገረ   የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው   \n",
       "6                               በነገራችን ላይ አንዳርጋቸው ፅጌ   \n",
       "7   ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ   \n",
       "8  የአሜሪካ ድምፅ ራዲዮ ከአቶ ንጉሱ ጥላሁን እና ከሆኑት አቶ ነቢያት ጌታቸ...   \n",
       "9                    ይድርስ በኢትዮጵያ ለምትገኙ የዩኒቨርስቲ ተማሪዎች   \n",
       "\n",
       "                                             content  \n",
       "0  የሀገራችን ህዝቦች በአብሮነትና በአንድነት በህብረ ብሄራዊነት ለበርካታ አ...  \n",
       "1  በታገቱ ተማሪዎችና በሌሎች የአገር ደህንነትና መከላከያ ጉዳዮች ዙሪያ አዴ...  \n",
       "2  ባህር ዳር ሰላማዊ ሰልፎቹ በደምቢ ዶሎ ዩኒቨርሲቲ ሲማሩ የነበሩና የታገቱ...  \n",
       "3  በኢፌዲሪ የውጭ ጉዳይ ሚኒስትር አቶ ገዱ አንዳርጋቸው የተመራ የኢትዮጵያ ...  \n",
       "4                            ጦቢያ ግጥምን በጃዝ  መላኩ ታረቀኝ   \n",
       "5       ዶክተር አብይ ሱዳንን አሻገረ  የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው   \n",
       "6                              በነገራችን ላይ አንዳርጋቸው ፅጌ   \n",
       "7  ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ   \n",
       "8  የአሜሪካ ድምፅ ራዲዮ የጠቅላይ ሚኒስትር ፅህፈት ቤት ፕሬስ ሴክሬታሪያት ...  \n",
       "9  በገክርስቶስ አባይጥር  ቀን  አመተ ምህረትከሁሉ አስቀድሜ ሰላምና ጤና እ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0152156c-b483-429e-a0ff-efa50d3236f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineModel:\n",
    "\n",
    "    am_sent_endings = r'\\?|\\!|\\።|\\፡፡'\n",
    "    am_punctuation = '፠፡።፣፤፥፦፧፨“”‘’…‹‹››·•'\n",
    "    am_numbers = '፩፪፫፬፭፮፯፰፱፲፳፴፵፶፷፸፹፺፻፼'\n",
    "    am_random = '�©\\uf0c4\\uf0d8\\uf0a7\\uf066\\uf0d8'\n",
    "    stop_words = open('stopwords.txt', encoding='utf-8').read().split()\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.raw_text = text\n",
    "        self.clean_text = None\n",
    "        self.sentences = None\n",
    "        self.words = None\n",
    "\n",
    "        self.process_text()\n",
    "\n",
    "    def process_text(self):\n",
    "        self.clean_text = self.clean_minimized(self.raw_text)\n",
    "        self.sentences = self.extract_sentences(self.clean_text)\n",
    "        self.sentences = self.remove_duplicate_sentence(self.sentences)\n",
    "        self.words = self.extract_words(self.clean_text)\n",
    "\n",
    "    def extract_sentences(self, text=None):\n",
    "        if text is None:\n",
    "            text = self.raw_text\n",
    "        sentences = re.split(self.am_sent_endings, text)\n",
    "        return sentences\n",
    "\n",
    "    def extract_words(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def clean_minimized(self, text):\n",
    "        words = text.split()\n",
    "        to_clean = string.punctuation + self.am_numbers + self.am_random + string.ascii_letters + string.digits + self.am_punctuation\n",
    "        to_clean = re.sub(self.am_sent_endings, '', to_clean)\n",
    "        table = str.maketrans('', '', to_clean)\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        clean_txt = list(filter(None, stripped))\n",
    "        return ' '.join(clean_txt)\n",
    "\n",
    "    def remove_duplicate_sentence(self, sentences):\n",
    "        duplicates = []\n",
    "        cleaned = []\n",
    "        for s in sentences:\n",
    "            if s in cleaned:\n",
    "                if s in duplicates:\n",
    "                    continue\n",
    "                else:\n",
    "                    duplicates.append(s)\n",
    "            else:\n",
    "                cleaned.append(s)\n",
    "        return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9e0207d-01a1-4f62-a561-8fa6aa738a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: etnltk in /home/yohannes/.local/lib/python3.10/site-packages (0.0.22)\n",
      "Requirement already satisfied: emoji>=1.7.0 in /home/yohannes/.local/lib/python3.10/site-packages (from etnltk) (2.9.0)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/yohannes/.local/lib/python3.10/site-packages (from etnltk) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in /home/yohannes/.local/lib/python3.10/site-packages (from textsearch>=0.0.21->etnltk) (2.0.0)\n",
      "Requirement already satisfied: anyascii in /home/yohannes/.local/lib/python3.10/site-packages (from textsearch>=0.0.21->etnltk) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install etnltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a5dd839-4442-44c4-8929-604505025d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_similarities(text):\n",
    "#     sentences = remove_stopwords(text)\n",
    "#     sentences = sent_tokenize(str(text))\n",
    "#     print(sentences)\n",
    " \n",
    "#     vectorizer = TfidfVectorizer()\n",
    "   \n",
    "#     trsfm = vectorizer.fit_transform(sentences)\n",
    "#     text_df = pd.DataFrame(trsfm.toarray(), columns=vectorizer.get_feature_names_out(), index=sentences)\n",
    "\n",
    "#     num_sentences = text_df.shape[0]\n",
    "#     num_summary_sentences = int(np.ceil(num_sentences ** 0.5))\n",
    "\n",
    "#     similarities = cosine_similarity(trsfm, trsfm)\n",
    "\n",
    "#     avgs = []\n",
    "#     for i in similarities:\n",
    "#         avgs.append(i.mean())\n",
    "\n",
    "    \n",
    "#     top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "\n",
    "#     return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e0feea9-be35-464f-b8a0-31ef6d2ab226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_summary(text):\n",
    "#     sents_for_sum = find_similarities(text)\n",
    "#     sort = sorted(sents_for_sum)\n",
    "   \n",
    "#     print('Number of selected sentences', len(sort))\n",
    "#     sent_list = sent_tokenize(str(text))\n",
    "#     print('Total number of sentences', len(sent_list))\n",
    "\n",
    "#     sents = []\n",
    "#     for i in sort:\n",
    "#         sents.append(sent_list[i].replace('\\n', '።') + '።')\n",
    "\n",
    "#     summary = ' '.join(sents)\n",
    "#     return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aed5639e-89e5-4ef4-b89b-df2875495d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(sentences, stopwords):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "    trsfm = vectorizer.fit_transform(sentences)\n",
    "    text_df = pd.DataFrame(trsfm.toarray(), columns=vectorizer.get_feature_names_out(), index=sentences)\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences ** .5))\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    " \n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "   \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6864d1a0-d597-412b-8bdd-75ce4af3a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary(sentences, stop_words):\n",
    "    sents_for_sum = find_similarities(sentences, stopwords=stop_words)\n",
    "    sort = sorted(sents_for_sum)\n",
    "    print('Number of selected sentences', len(sort))\n",
    "    sent_list = sentences\n",
    "    print('Total number of sentences', len(sent_list))\n",
    "   \n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', '') + '።')\n",
    "    summary = ' '.join(sents)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c256b484-67cf-4cb3-84d2-f2830e338535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import etnltk\n",
    "from etnltk.tokenize.am import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f89ede2e-ffb3-4c64-8b5a-c9392cc97eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def longest_common_subsequence(a, b):\n",
    "    matcher = SequenceMatcher(None, a, b)\n",
    "    match = matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "    return a[match.a: match.a + match.size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0f3537c-fe3f-4517-8542-5e9add190977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(reference, candidate, n=1):\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    candidate_tokens = word_tokenize(candidate)\n",
    "    \n",
    "    reference_ngrams = set(nltk.ngrams(reference_tokens, n))\n",
    "    candidate_ngrams = set(nltk.ngrams(candidate_tokens, n))\n",
    "    \n",
    "    precision = len(reference_ngrams.intersection(candidate_ngrams))\n",
    "    recall = len(reference_ngrams) + len(candidate_ngrams) - precision\n",
    "    \n",
    "    if recall == 0:\n",
    "        rouge_n_score = 0.0\n",
    "    else:\n",
    "        rouge_n_score = precision / recall\n",
    "    \n",
    "    return rouge_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec942cef-992f-43e8-80e9-0957dbc6b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_l(reference, candidate):\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    candidate_tokens = word_tokenize(candidate)\n",
    "    \n",
    "    reference_ngrams_all = list(nltk.ngrams(reference_tokens, 1))\n",
    "    candidate_ngrams_all = list(nltk.ngrams(candidate_tokens, 1))\n",
    "    \n",
    "    reference_lcs = longest_common_subsequence(reference_ngrams_all, candidate_ngrams_all)\n",
    "    \n",
    "    rouge_l_score = len(reference_lcs) / max(1, len(reference_ngrams_all))\n",
    "    \n",
    "    return rouge_l_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad53d2ac-0190-4fdb-997a-cdbb55d25f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(reference, candidate):\n",
    "    rouge_n_1 = rouge_n(reference, candidate, n=1)\n",
    "    rouge_n_2 = rouge_n(reference, candidate, n=2)\n",
    "    rouge_l_score = rouge_l(reference, candidate)\n",
    "    \n",
    "    return rouge_n_1, rouge_n_2, rouge_l_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc4c827c-d419-446f-a4a7-ed9c52b2fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(reference_summary, generated_summary):\n",
    "    rouge_n_1, rouge_n_2, rouge_l_score = calculate_rouge_scores(reference_summary, generated_summary)\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_n_1:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_n_2:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fd98cff-cb22-4d99-bc24-781d2f5b71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_data['content']\n",
    "y = cleaned_data['title']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa850467-e9ff-46c1-b74b-a9cf55920999",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94764837-64fc-47be-9e49-d565e6c4c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected sentences 5\n",
      "Total number of sentences 20\n",
      "ROUGE-1: 0.9000\n",
      "ROUGE-2: 0.8235\n",
      "ROUGE-L: 0.7049\n",
      "\n",
      "\n",
      "\n",
      "labeled summary \n",
      " በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች። \n",
      " ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል።  \n",
      " በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች።  ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n",
      "       \n",
      "\n",
      "\n",
      "Generated Summary በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች።  ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል።  በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች።  የናሚቢያው ግብ ጠባቂ ኔብሉ በኛው ደቂቃ በቀጥታ ቀይ ካርድ ከሜዳ ተሰናብቷል።  ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n"
     ]
    }
   ],
   "source": [
    "content = '''\n",
    "በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች። ደካማ የሚባል አቋም ያሳየችው ካሜሩን ሻንጣዋን ልትሸክፍ ግድ ሆኗል። የጣሊያኑ አታላንታ አጥቂ ለአጋማሽ ዘጠኝ ደቂቃዎች ሲቀሩት ባስቆጠራት ጎል ሀገሩን ቀዳሚ አድርጓል። \n",
    "ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል። የቀድሞው ከ21 ዓመት በታች የእንግሊዝ ብሔራዊ ቡድን አጥቂ የነበረው ሉክማን ሁለተኛዋን ጎሎ በ90ኛው ደቂቃ በማስቆጠር የካሜሩንን ልብ ሰብሯል።\n",
    "ምንም እንኳ በጉዳት ምክንያት ሳይሰለፍ የቆየው የካሜሩኑ አጥቂ ቪንሴንት አቡበከር ዘግይቶ ወደ ሜዳ ቢገባም ሀገሩ አንድ ዒላማውን የጠበቀ ኳስ ሳታስመዘግብ ወጥታለች። አቡበከር ባለፈው ካሜሩን ባዘጋጀችው የ2021 የአፍሪካ ዋንጫ ኮከብ ጎል አስቆጣሪ ነበር። በዓለም ዋንጫ ከምድቡ ማለፍ ያልቻለው የካሜሩኑ አሠልጣኝ ሪጎቤርት ሶንግ ጫናው እየበረታ መጥቷል።\n",
    "የካሜሩን እግር ኳስ ፌዴሬሽን ፕሬዝደንት ሳሙዔል ኤቶ ጨዋታውን በሥፍራው ተገኝቶ የተከታተለ ሲሆን የቀድሞ የቡድን አጋሩን ሥራ የመታደግ አሊያም የማባረር ኃላፊነት አለበት። በዘንድሮው ውድድር እምብዛም አስደናቂ ብቃት ያላሳዩት የማይበገሩት አናብስት ከናይጄሪያ በነበራቸውም ጨዋታው ከቅርፅ ውጭ ሆነው ተስተውለዋል።\n",
    "በተቃራኒው ናይጄሪያ ያገኘችው ውጤት የሚገባት የሚያስብል ሲሆን ተጋጣሚዋን በልጣ ታይታለች። በዘጠነኛው ደቂቃ የናይጄሪያው ተከላካይ ሴሚ አጃይ ያስቆጠራት ጎል በቪኤአር ታይታ ውድቅ ሆናበታለች።\n",
    "የካሜሩንን የመጀመሪያ ጨዋታ ለማንቸስተር ዩናይትድ ለመጫወት ሲል የቀጣው ግብ ጠባቂው አንድሬ ኦናና ለቀሪዎቹ ጨዋታዎች በግል አውሮፕላን ወደ አይቮሪ ኮስት ቢበርም መሰለፍ የቻለው አንድ ጨዋታ ብቻ ነው። እሱን ተክቶ የተሰለፈው ኦንዶዋ ስኅተት በፈፀመ ቁጥር ካሜራው ኦናናን ሲያስመልክት ተስተውሏል።\n",
    "በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን 3 ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች። የናሚቢያው ግብ ጠባቂ ኔብሉ በ17ኛው ደቂቃ በቀጥታ ቀይ ካርድ ከሜዳ ተሰናብቷል። ጌልሰን ዳላ ከ9 ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n",
    "ዳላ ለራሱ እና ለሀገሩ ሁለተኛዋን ጎል ሲያስቆጥር ማቡሉሉ ደግሞ በአስደናቂ ሁኔታ ሶስተኛዋን በማከል አንጎላ ሩብ ፍፃሜውን እንድትቀላቀል አስችለዋል። አንጎላ በመጪው አርብ በሩብ ፍፃሜው ናይጄሪያን ትገጥማለች።\n",
    "        '''\n",
    "\n",
    "title  = '''\n",
    " በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች። \n",
    " ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል።  \n",
    " በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች።  ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n",
    "       '''\n",
    "tparser = CosineModel(content)\n",
    "article_summary = build_summary(tparser.sentences, tparser.stop_words)\n",
    "rouge_score(title, article_summary)\n",
    "print(\"\\n\\n\")\n",
    "print(\"labeled summary\", title)\n",
    "print(\"\\n\")\n",
    "print(\"Generated Summary\", article_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "953e1a8b-8fd1-4139-8adc-f4d37b38a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected sentences 4\n",
      "Total number of sentences 11\n",
      "\n",
      "\n",
      "\n",
      "Text\n",
      "\n",
      "\n",
      "ተዘግቶ የነበረው መጋረጃ ሲገለጥ የሚል ርእስ የሰጠሁበት ዋናው ምክንያት እነ ሻምበል አምሃ አበበና የመሳሰሉት በመቶዎች የሚቆጠሩ ብዙ ወጣት መኮንኖች የበታች ሹሞችና ወታደሮች በኮርስ ወይም በቅርብ ጓደኞቻቸው የሀሰት ጥቆማ ተካሂዶባቸው ህይወታቸው የተቀጨበት አካለ ስንኩላን የሆኑበት ምክንያት ብዙ ሚስጢር መውጣት በመጀመሩ ነው። ኮሎኔል መርሻ ወዳጆ በመጽሐፉ ሽፋን ላይ ስለመጽሐፉ የሚከተለውን አስተያየት ሰጥተዋል። ጄኔራል አማን የጦር አለቆችን ማነጋገር መጀመራቸው የደርግ አባላቱ ሲረዱ የፍርሃትና የጭንቀት ብርድ ውስጥ ከተታቸው። ደህንነቱ ልዩ ልዩ የጦር ክፍሎችና ከፖሊስ ሠራዊት ተዛውረን የመጣን ሲሆን ከመሥሪያ ቤቴ የሰው ኃይል ሲነፃፀር ብዛት ባይኖራቸውም በተለያዩ የትምህርት ዘርፎች በዶክትሬት በማስትሬት በባችለር ዲግሪ ከዚያም ዝቅ ሲል በዲፕሎማ የተመረቁ በተጨማሪ ብዛት ያላቸው ሠራተኞች ወደ ተለያዩ ሶሻሊስት አገሮች ተልከው የመረጃና የሶሻሊስት ርእዮተ ዓለም ትምህርት የተክታተሉ አባሎች እንደነበሩ እሙን ነው። በሰሜን በኩል የነበረው የኢትዮጵያ ጦር መፈረካከስና መበተን የጀመረው ከግንቦት ቀን በኋላ እንደሆነ ግልጽ ነው። አስተያየት በእርግጥም የውጭ መረጃ ሠራተኞች ጂቡቲ ሆነው የሱማሌን መንግሥት የጦር ቢሮ ቦርቡረው ውጤት አስገኝተዋል የሚለው እውነትነት ካለው ሁላችንንም ማለትም በደህነነት መቤት ውስጥ የነበርነውን ሠራተኞች ሁሉ የሚያኮራን ነው። ከ ዓም ወዲህ የሻዕቢያና የ ወያኔ ቡድን አባሎች በጋራና በተናጠል አገር ውስጥ በስውር ይንቀሳቀሱ እንደነበር በተመሳሳይ ጂቡቲ ኬኒያ ሱማሌና በተለይም ሱዳን ውስጥ ጽሕፈት ቤቶቻቸውን ከፍተው በከፍተኛ ደረጃ ፀረ ኢትዮጵያ እንቅስቃሴ ሲያደርጉ እንደነበር ይታወቃል። ሻምበል ተስፋዬ ርስቴ በጻፉት መጽሐፍ በገጽ የውጭ አገሮች ጥናትና ምርምር ድርጅት ውስጥ እየተገናኙ ውይይት ያደርጉ እንደነበር ገልጸዋል። ስለዚህ የፕሬዚዳንት መንግሥቱ ኃማርያም ማንነትና የሚስጥር አማካሪዎቻቸው እነማን እንደነበሩ ከላይ የተገለጸው ጥሩ ማስረጃ ነው እላለሁ። በእርግጥ ከላይ ስማችሁ የተገለጸው መኮንኖች የአንድ ኮርስ መኮንኖች መሆናችሁን በወቅቱ ሻምበል ቁምላቸው ተካና ሻምበል አመሀ አበበ አንድ ቤት ውስጥ ተከራይተው ይኖሩ እንደነበር ይነገራል።\n",
      "===================\n",
      "Summarized\n",
      "\n",
      "\n",
      " ደህንነቱ ልዩ ልዩ የጦር ክፍሎችና ከፖሊስ ሠራዊት ተዛውረን የመጣን ሲሆን ከመሥሪያ ቤቴ የሰው ኃይል ሲነፃፀር ብዛት ባይኖራቸውም በተለያዩ የትምህርት ዘርፎች በዶክትሬት በማስትሬት በባችለር ዲግሪ ከዚያም ዝቅ ሲል በዲፕሎማ የተመረቁ በተጨማሪ ብዛት ያላቸው ሠራተኞች ወደ ተለያዩ ሶሻሊስት አገሮች ተልከው የመረጃና የሶሻሊስት ርእዮተ ዓለም ትምህርት የተክታተሉ አባሎች እንደነበሩ እሙን ነው።  አስተያየት በእርግጥም የውጭ መረጃ ሠራተኞች ጂቡቲ ሆነው የሱማሌን መንግሥት የጦር ቢሮ ቦርቡረው ውጤት አስገኝተዋል የሚለው እውነትነት ካለው ሁላችንንም ማለትም በደህነነት መቤት ውስጥ የነበርነውን ሠራተኞች ሁሉ የሚያኮራን ነው።  ሻምበል ተስፋዬ ርስቴ በጻፉት መጽሐፍ በገጽ የውጭ አገሮች ጥናትና ምርምር ድርጅት ውስጥ እየተገናኙ ውይይት ያደርጉ እንደነበር ገልጸዋል።  በእርግጥ ከላይ ስማችሁ የተገለጸው መኮንኖች የአንድ ኮርስ መኮንኖች መሆናችሁን በወቅቱ ሻምበል ቁምላቸው ተካና ሻምበል አመሀ አበበ አንድ ቤት ውስጥ ተከራይተው ይኖሩ እንደነበር ይነገራል።\n"
     ]
    }
   ],
   "source": [
    "# input_text = '''\n",
    "# ከተለያዩ የትግራይ አካባቢዎች በመምጣት ኑሯቸውን በመቀሌ ያደረጉ በርካታ ወጣቶች፣ በከተማዋ ምንም ዓይነት የሥራ ዕድል ባለመኖሩና ከሥራ ውጪ በመሆናቸው  ከጠዋት እስከ ማታ ጊዜያቸውን ቁማር ቤት እንደሚያሳልፉ ይናገራሉ።\n",
    "\n",
    "# ወጣቶቹ በተለይ በጦርነቱ ምክንያት ወላጀ አልባ ለሆኑ ሕፃናትና ጧሪ ቀባሪዎቻቸውን ላጡ አቅመ ደካማ አረጋውያን ብቸኛ አስተዳዳሪ በመሆናቸው፣ የእነሱን እጅ የሚጠብቁ ቤተሰቦቻቸውን ሠርተው በቀን አንዴ እንኳን መመገብ አለመቻላቸው፣ ቤት ውስጥ ተቀምጠው የቤተሰቦቻቸውን መራብ ማየት ከባድ ስለሆነባቸው ውሏቸውን በቢንጎ፣ በፑል ቤት፣ በካርታ፣ በስፖርታዊ ጨዋታዎች ውርርድና በሌሎች የቁማር ዓይነቶች ለማድረግ መገደዳቸውን ገልጸዋል።\n",
    "\n",
    "# የመቀሌ ከተማ ነዋሪ የሆነው ወጣት ልዑል ኃይለ፣ ‹‹በትግራይ የጥይት ድምፅ ጠፋ እንጂ ረሃብና የኑሮ ውድነት ከሥራ አጥነት ጋር ተዳምሮ ወጣቱ ተስፋ በመቁረጡ፣ ለስርቆትና ለቁማር እየተዳረገ ስለሆነ የፌዴራል መንግሥት ሊያየን ይገባል፤›› ብሏል።\n",
    "\n",
    "# ወጣቱ አክሎም ትግራይ ክልል ከጦርነቱ ማግሥት አንፃራዊ ሰላም ቢገኝም፣ ጦርነቱ ባስከተለው ጉዳት ምክንያት አብዛኞቹ ፋብሪካዎች ከሥራ ውጪ በመሆናቸው፣ የሚጀመሩ የኮንስትራክሽን ሥራዎች ባለመኖራቸውና የተጀመሩትም ባሉበት በመቆማቸው የቀን ሥራ ማግኘት እንኳን ለወጣቱ ብርቅ እንደሆነበት ተናግሯል።\n",
    "\n",
    "# መቀሌን ጨምሮ በክልሉ የተለያዩ ከተሞች የሚታዩ የሌብነትና የዘረፋ ወንጀሎች ከዚሁ ከሥራ አጥነትና ተስፋ መቁረጥ ጋር ተያይዘው እየተስፋፉ ነው ብሏል።\n",
    "# '''\n",
    "input_text = \"ተዘግቶ የነበረው መጋረጃ ሲገለጥ የሚል ርእስ የሰጠሁበት ዋናው ምክንያት እነ ሻምበል አምሃ አበበና የመሳሰሉት በመቶዎች የሚቆጠሩ ብዙ ወጣት መኮንኖች የበታች ሹሞችና ወታደሮች በኮርስ ወይም በቅርብ ጓደኞቻቸው የሀሰት ጥቆማ ተካሂዶባቸው ህይወታቸው የተቀጨበት አካለ ስንኩላን የሆኑበት ምክንያት ብዙ ሚስጢር መውጣት በመጀመሩ ነው። ኮሎኔል መርሻ ወዳጆ በመጽሐፉ ሽፋን ላይ ስለመጽሐፉ የሚከተለውን አስተያየት ሰጥተዋል። ጄኔራል አማን የጦር አለቆችን ማነጋገር መጀመራቸው የደርግ አባላቱ ሲረዱ የፍርሃትና የጭንቀት ብርድ ውስጥ ከተታቸው። ደህንነቱ ልዩ ልዩ የጦር ክፍሎችና ከፖሊስ ሠራዊት ተዛውረን የመጣን ሲሆን ከመሥሪያ ቤቴ የሰው ኃይል ሲነፃፀር ብዛት ባይኖራቸውም በተለያዩ የትምህርት ዘርፎች በዶክትሬት በማስትሬት በባችለር ዲግሪ ከዚያም ዝቅ ሲል በዲፕሎማ የተመረቁ በተጨማሪ ብዛት ያላቸው ሠራተኞች ወደ ተለያዩ ሶሻሊስት አገሮች ተልከው የመረጃና የሶሻሊስት ርእዮተ ዓለም ትምህርት የተክታተሉ አባሎች እንደነበሩ እሙን ነው። በሰሜን በኩል የነበረው የኢትዮጵያ ጦር መፈረካከስና መበተን የጀመረው ከግንቦት ቀን በኋላ እንደሆነ ግልጽ ነው። አስተያየት በእርግጥም የውጭ መረጃ ሠራተኞች ጂቡቲ ሆነው የሱማሌን መንግሥት የጦር ቢሮ ቦርቡረው ውጤት አስገኝተዋል የሚለው እውነትነት ካለው ሁላችንንም ማለትም በደህነነት መቤት ውስጥ የነበርነውን ሠራተኞች ሁሉ የሚያኮራን ነው። ከ ዓም ወዲህ የሻዕቢያና የ ወያኔ ቡድን አባሎች በጋራና በተናጠል አገር ውስጥ በስውር ይንቀሳቀሱ እንደነበር በተመሳሳይ ጂቡቲ ኬኒያ ሱማሌና በተለይም ሱዳን ውስጥ ጽሕፈት ቤቶቻቸውን ከፍተው በከፍተኛ ደረጃ ፀረ ኢትዮጵያ እንቅስቃሴ ሲያደርጉ እንደነበር ይታወቃል። ሻምበል ተስፋዬ ርስቴ በጻፉት መጽሐፍ በገጽ የውጭ አገሮች ጥናትና ምርምር ድርጅት ውስጥ እየተገናኙ ውይይት ያደርጉ እንደነበር ገልጸዋል። ስለዚህ የፕሬዚዳንት መንግሥቱ ኃማርያም ማንነትና የሚስጥር አማካሪዎቻቸው እነማን እንደነበሩ ከላይ የተገለጸው ጥሩ ማስረጃ ነው እላለሁ። በእርግጥ ከላይ ስማችሁ የተገለጸው መኮንኖች የአንድ ኮርስ መኮንኖች መሆናችሁን በወቅቱ ሻምበል ቁምላቸው ተካና ሻምበል አመሀ አበበ አንድ ቤት ውስጥ ተከራይተው ይኖሩ እንደነበር ይነገራል።\"\n",
    "# input_text = \"በኢትዮጵያ በጥሬ ገንዘብ የሚደረጉ የገንዘብ እንቅስቃሴዎች እየቀነሱ መሆኑን መረጃዎች እያመለከቱ ነው፡፡ በአብዛኛዎቹ የኢትዮጵያ ባንኮች የሚደረጉ የገንዘብ እንቅስቃሴዎች በተለያዩ የዲጂታል አማራጮች እየተተገበሩ ነው፡፡ በተለይ በ2015 ሒሳብ ዓመት እንደ ሪከርድ የሚቆጠር አፈጻጸም መታየቱም ይገልጻል፡፡ በ2016 ሒሳብ ዓመትም ቢሆን በዲጂታል ባንክ አገልግሎት የሚንቀሳቀሱ የገንዘብ መጠን እያደገ ስለመምጣቱ እየወጡ ያሉ መረጃዎች እያመላከቱ ነው፡፡ በጥሬ ገንዘብ የሚደረግ ግብይት በከፍተኛ ደረጃ እየቀነሰ መምጣቱ ተጠቆመ:: የኢትዮጵያ ብሔራዊ ባንክ ይፋዊ መረጃ እንደሚያሳየውም፣ በተጠናቀቀው የ2015 ሒሳብ ዓመት በዲጂታል የክፍያ አማራጮች የተንቀሳቀሰው የገንዘብ መጠን ከ4.7 ትሪሊዮን ብር በላይ ደርሷል፡፡ ይህ የገንዘብ መጠን ከቀዳሚው ዓመት የ2014 የሒሳብ ዓመት ጋር ሲነፃፀር፣ ከ3.1 ትሪሊዮን ብር በላይ ብልጫ አለው፡፡ በግብይት መጠንም ቢሆን በ2014 የሒሳብ ዓመት የተፈጸመው ትራንዛክሽን ወይም ግብይት 345.7 ሚሊዮን ሲሆን፣ በ2015 ሒሳብ ዓመት ግን 1.24 ቢሊዮን ደርሷል፡፡ \"\n",
    "# input_text = \"በኢትዮጵያ ንግድ ባንክ መረጃ መሠረት በ2016 የሒሳብ ዓመት ግማሽ ዓመት ብቻ ከ2.8 ትሪሊዮን ብር በላይ በዲጂታል የባንክ አገልግሎት ገንዘብ ማንቀሳቀስ ችሏል፡፡ ይህ ገንዘብ እንቅስቃሴ ከ494 ሚሊዮን ብር በላይ በሚሆኑ ግብይቶች ወይም ትራንዛሽኖች የተፈጸመ ነው፡፡ ይህ አፈጻጸም ከቀዳሚው ዓመት ተመሳሳይ ወቅት ጋር ሲነፃፀር፣ በግብይት የ35 በመቶ በገንዘብ መጠን ደግሞ የ118 በመቶ ዕድገት የታየበት ነው፡፡ በቀዳሚው ዓመት በሙሉ የሒሳብ ዓመት 3.2 ትሪሊዮን ብር ማንቀሳቀስ ችሎ የነበረው የኢትዮጵያ ንግድ ባንክ፣ የስድስት ወራት 2.8 ቢሊዮን ብር ማንቀሳቀስ የቻለው ከ12 ሚሊዮን በላይ የኤቲኤም ተጠቃሚ፣ ከዘጠኝ ሚሊዮን በላይ የሞባይል ባንክና ከ20 ሚሊዮን ብር በላይ በሚሆኑ ደንበኞች በመያዝ እንደሆነ ታውቋል፡፡\"\n",
    "\n",
    "tparser = CosineModel(input_text)\n",
    "article_summary = build_summary(tparser.sentences, tparser.stop_words)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Text\")\n",
    "print(\"\\n\")\n",
    "print(input_text)\n",
    "print(\"===================\")\n",
    "print(\"Summarized\")\n",
    "print(\"\\n\")\n",
    "print(article_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3807f50-b81a-4390-bae8-f894ddfd5b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
